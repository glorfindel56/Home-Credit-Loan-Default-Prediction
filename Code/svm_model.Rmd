---
title: "svm model"
output:
  html_document: default
  word_document:
    fig_caption: yes
    fig_height: 5
    fig_width: 5
    reference_docx: hw_template.docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kernlab)
library(caret)
library(dplyr)
library(tidyverse)
library(kernlab)
library(ggplot2)
library(GGally)
library(factoextra)
```

importing data

```{r importing data,results='hide',message=FALSE,cache=TRUE}
data <-read_csv(file = "/Users/joycehu/Library/CloudStorage/Box-Box/MGT 6203/application_imputed.csv", col_names = TRUE)

colnames(data) <- str_to_lower(colnames(data))

# removing certain columns
data <- data %>%
  select(-c(has_children, married, has_children_num, education_level_num, 
            age_in_years_num, married_num, code_gender_num, income_bracket_num))

# adding feature engineering columns 
# changing any NAs to a different name for modeling
data <- data %>%
  mutate(occupation_type_adj = replace_na(occupation_type, "Unknown"),
         credit_to_income_ratio = amt_credit / amt_income_total,
         credit_to_annuity_ratio = amt_credit / amt_annuity,
         credit_to_goods_price_ratio = amt_credit / amt_goods_price)

# setting up the data to be used for modeling
# setting the categorical variables as factors
# making sure the factor levels are valid R variable names
# removing any "repetitive" data
data <- data %>%
  mutate(target = as.factor(target),
         name_contract_type = as.factor(name_contract_type),
         code_gender = as.factor(code_gender),
         flag_own_car = as.factor(flag_own_car),
         name_education_type = as.factor(name_education_type),
         name_family_status = as.factor(name_family_status),
         occupation_type_adj = as.factor(occupation_type_adj),
         education_level = as.factor(education_level)) %>%
  mutate(target = fct_recode(target, "No" = "0", "Yes" = "1")) %>%
  select(-c(days_birth, # keeping year version instead
            days_employed, # keeping year version instead
            occupation_type # accounted for NAs in occupation_type_adj
            ))

# checking data type
str(data)
```

#### I'll be doing K-folds cross validation to find a good classifier for SVM model

1. Creating the training/validation and testing data sets and set the final_test_data aside to test the model picked by K-fold CV.

```{r}
#let's set a seed for reproducibility
set.seed(5678) #random number

# joyce code comment (from line 40 onwards)
# possible future enhancement is to try oversampling
# first, separate dataset based on target = 0 and target = 1
data_0 <- data %>% 
  filter(target == "No")
data_1 <- data %>%
  filter(target == "Yes")

# separate the dataset into 80/20 (80% training and validation for cross validation and 20% for test)
n_0 <- sum(data$target == "No")
n_1 <- sum(data$target == "Yes")
split_value <- 0.80

#we are randomly shuffling the entire dataset and then splitting it up according the the split_value we set above.
training_valid_data_points_0 <- sample(x = 1:n_0, size = as.integer(split_value*n_0), replace = FALSE)
training_valid_data_points_1 <- sample(x = 1:n_1, size = as.integer(split_value*n_1), replace = FALSE)

# subsetting the data based on target = 0 and target = 1 to get the same distribution for the target variable in 
# the train/validation/test datasets

# train/validation dataset that will be used for k-fold cross-validation
train_valid_data_0 <- data_0[training_valid_data_points_0, ]
train_valid_data_1 <- data_1[training_valid_data_points_1, ]
# merging the separate train/validation datasets into one
train_valid_data <- bind_rows(train_valid_data_0, train_valid_data_1)

# final dataset that will be used to analyze how well the best model (from k-fold cross-validation) performs
final_test_data_0 <- data_0[-training_valid_data_points_0, ]
final_test_data_1 <- data_1[-training_valid_data_points_1, ]
# merging the separate test datasets into one
final_test_data <- bind_rows(final_test_data_0, final_test_data_1)

#need to scale data (standardize and normalize)

# remove unneeded datasets to clear up memory space
rm(data_0)
rm(data_1)
rm(train_valid_data_0)
rm(train_valid_data_1)
rm(final_test_data_0)
rm(final_test_data_1)
```

2. Set up k-folds cross validation. In 6501, Professor Sokol mentioned k=10 is a good value to use. It's not necessarily always the most optimal but smaller values of k (e.g \<5) can lead to higher variance in performance estimate because the evaluation is based on fewer data points which larger k's (\>20) can lead to higher bias in the estimate because each fold contains a smaller portion of the data.

First, I define the function for running svm and obtaining the accuracy.

```{r, svm, message=FALSE}
library(pROC)

# setting seed for reproducibility
set.seed(5678)

# define the function for running SVM
svm_model <- function(formula, method_input, metric_input) {
# Define custom summary function for ROC
# Define your k-fold cross-validation control
control <- trainControl(method = "cv", number = 2, classProbs = TRUE)
# metric <- "Accuracy"
model <- train(formula, data = train_valid_data, method = method_input, 
                tuneLength = 5, preProcess = c("center", "scale"),  
                metric = metric_input, trControl = control)
}

# Support Vector Machines with Radial Basis Function Kernel
# tuning sigma (sigma) and c (cost)
# Sigma in support vector machines helps decide how closely the lines are drawn to the dots.
# If you choose it just right, you can separate the dots into groups really well!)
# Think back to visual graphs that Dr. Sokol used to teach us SVM in ISYE 6501!
svm_1 <- svm_model(target ~ name_contract_type + code_gender + flag_own_car + cnt_children + amt_income_total +
  amt_credit + amt_annuity + amt_goods_price + name_education_type + name_family_status + 
  ext_source_1 + ext_source_2 + obs_30_cnt_social_circle + def_30_cnt_social_circle + 
  obs_60_cnt_social_circle + def_60_cnt_social_circle + days_last_phone_change + amt_req_credit_bureau_year +
  education_level + age_in_years + age_bucket + employed_in_years + income_bracket + 
  occupation_type_adj + credit_to_income_ratio + credit_to_annuity_ratio + 
  credit_to_goods_price_ratio, method_input = "svmRadial", metric_input = "Accuracy")

summary_func <- function(data, lev = NULL, model = NULL) {
  auc_result <- roc(predict(model, data), as.numeric(data$Class))
  auc_value <- auc(auc_result)
  out <- c(auc = auc_value)
  names(out) <- "AUC"
  out
}

# variables that i want to test separately and then together based on how similar they are to each other
# name_education_type, education_level
# age_in_years, age_bucket
# amt_income_total, income_bracket
# obs_30_cnt_social_circle, obs_60




```

Then I define the k-folds function. This returns a table that contains each k nearest neighbor hyperparameter, the k-folds validation, and accuracy per portion of k-folds validation. This table is then averaged on the accuracy across k-folds evaluations. Then, we can see what the average accuracy of each model is to determine which knn k value is best to use. I will be using k-folds=10 as stated above and testing knn models using k nearest neighbor values from 200-600, incrementing by 20.

```{r}
kfolds_CV <- function(k_folds,train_valid_data,k_neighbors, kernel){
  #create a df to store the results
  results <- data.frame()
  #shuffling the dataset
  set.seed(5678) #random number
  train_valid_data<-train_valid_data[sample(nrow(train_valid_data)),]
  #segmenting my training/validating data into k folds.
  folds <- cut(seq(1,nrow(train_valid_data)),breaks=k_folds,labels=FALSE)
 
  #determine the loop for how many k_neighbors I want to try
  k_neighbors <- seq(200, 600, by = 20)
  for(kn in k_neighbors){
      #Perform k fold cross validation
     for(kf in 1:k_folds){
      #Segment data by fold using the which() function 
      test_indexes <- which(folds==kf,arr.ind=TRUE)
      #define the test data per the indices defined above
      test_data <- train_valid_data[test_indexes, ]
      #same for train data
      train_data <- train_valid_data[-test_indexes, ]
      #run the knn function and get the accuracy value for the above sets
      knn_res <- knn_accuracy(train_data,test_data,
                              k = kn,kernel = "optimal")
      #staging the results to add to final df
      result <- data.frame(current_k_fold=kf, 
                           k_neighbor=kn, 
                           accuracy=knn_res)
      #add results to final df
      results <- bind_rows(results,result)
     }
  }
  return(results)
}
```

run the kfolds CV function above with parameters: k-folds=10, on the training/validating data set (80% of whole dataset), and using k of 1-20 for knn model with the kernel 'optimal'.

```{r, fig.height=4,fig.width=6}
model <- kfolds_CV(k_folds = 10,
                  train_valid_data = train_valid_data,
                  k_neighbors = 20,
                  kernel = "optimal")

#take the average of all 10 evaluations per k neighbor value and sort by highest mean accuracy
result <- model %>%
  group_by(k_neighbor) %>%
  summarize(mean_accuracy = mean(accuracy)) %>% 
  arrange(desc(mean_accuracy))

#group the mean accuracy value by k neighbors
result_grouped <- result %>%
  group_by(mean_accuracy) %>%
  summarize(k_neighbors_list = paste0(k_neighbor,collapse = ", ")) %>% 
  arrange(desc(mean_accuracy))
result_grouped

#save these values for testing below.
best_k_neighbor <- result$k_neighbor[1]
best_mean_accuracy <- result$mean_accuracy[1]
best_k_neighbors <- result_grouped$k_neighbors_list[1]

#plot the results
ggplot(result, aes(x = k_neighbor, y = mean_accuracy, color = k_neighbor, label = k_neighbor)) + geom_point() + ggtitle("K-Folds Cross Validation Accuracy") +
  labs(x = "K-nearest neighbor", y = "Mean Accuracy") + theme_minimal() +
  theme(legend.position = "none") +   scale_x_continuous(breaks = scales::pretty_breaks(n = 20))
```

So according to the "result_grouped" table, we can see that the k values for kknn that performed the best via 10-folds cross validation were `r best_k_neighbors` with a mean accuracy across 10-folds of `r best_mean_accuracy`. We will pick this model. Moving on to the final portion of the cross validation phase, evaluating this model on the testing data we portioned out at the beginning.

3.  Train chosen model on all of train_valid_data and evaluate on test_data. Then report the accuracy as it is performance on test_data.

```{r}
cv_test_accuracy <- knn_accuracy(training_data = train_valid_data,
                                 testing_data = final_test_data,
                                 k = best_k_neighbor,
                                 kernel = "optimal")
cv_test_accuracy
best_mean_accuracy

accuracy_diff <- abs(best_mean_accuracy - cv_test_accuracy)
accuracy_diff
```

It was expected that the true model accuracy `r cv_test_accuracy` is lower than the training accuracy `r best_mean_accuracy`...however it's still rather close being `r accuracy_diff` off from the training accuracy. I'm satisfied with choosing k value of `r best_k_neighbor` for this classifier.

### (b) splitting the data into training, validation, and test data sets (pick either KNN or SVM; the other is optional).

1.  creating the training,validation,and testing datasets. I want the majority of the data for training so I'll do 60% for training, 20% for validation, and 20% for test. First I split it up into 60/40. Then I split that last 40 in half for validation and test.

```{r, regular validation, message=FALSE}
#let's set a seed for reproducability
set.seed(1234) #random number

#first, separate the dataset into 60/20/20 (60% training, 20% validation, and 20% for test)
n_dp <- nrow(app_data)
training_split_value <- 0.60

#we are randomly shuffling the entire dataset and then splitting it up according the the training_split_value we set above.
training_data_points <- sample(x=1:n_dp,
                               size = as.integer(training_split_value*n_dp),
                               replace = FALSE)

#and then subsetting the data as such
regular_train_data <- app_data[training_data_points,]
fourty_perc_of_data <- app_data[-training_data_points,]

#and then divide the rest of the data evenly between validation and test
n_rest_of_data <- nrow(fourty_perc_of_data)
valid_split_value <- 0.5
set.seed(1234) #set seed again so I can reproduce this later
valid_data_points <- sample(x=1:n_rest_of_data,
                            size = as.integer(valid_split_value*n_rest_of_data),
                            replace = FALSE)

regular_valid_data <- fourty_perc_of_data[valid_data_points,]
regular_test_data <- fourty_perc_of_data[-valid_data_points,]
```

2.  train all models on regular_train_data and validate all models on regular_valid_data and pick the best model

```{r, knn regular validation,message=FALSE, fig.height=4,fig.width=6}
#define function to run regular cross validation
regular_v <- function(train_data,validation_data,k_neighbors, kernel){
  #create a df to store the results
  results <- data.frame()
  #determine the loop for how many k_neighbors I want to try
  for(kn in 1:k_neighbors){
      #run the knn function and get the accuracy value for validation set
      knn_res <- knn_accuracy(train_data,validation_data,
                              k = kn,kernel = kernel)
      #staging the results to add to final df
      result <- data.frame(k_neighbor=kn, accuracy=knn_res)
      #add results to final df
      results <- bind_rows(results,result)
  }
  return(results)
}
#run the regular validation function above with parameters: k-folds=10, on the training/validating data set (80% of whole dataset), and using k of 1-20 for knn model.
model <- regular_v(train_data = regular_train_data,
                    validation_data = regular_valid_data,
                    k_neighbors = 20,
                    kernel = "optimal")

result <- model %>%
  arrange(desc(accuracy))

#group the accuracy value by k neighbors
result_grouped <- result %>%
  group_by(accuracy) %>%
  summarize(k_neighbors_list = paste0(k_neighbor,collapse = ", ")) %>% 
  arrange(desc(accuracy))
result_grouped

#save these values for testing below.
best_k_neighbor <- result$k_neighbor[1]
best_accuracy <- result$accuracy[1]
best_k_neighbors <- result_grouped$k_neighbors_list[1]

#plot the results
ggplot(result, aes(x = k_neighbor, y = accuracy, color = k_neighbor, label = k_neighbor)) +
  geom_point() + ggtitle("Regular Validation Accuracy") + 
  labs(x = "K-nearest neighbor", y = "Accuracy") + theme_minimal() +
  theme(legend.position = "none") + 
  scale_x_continuous(breaks = scales::pretty_breaks(n = 20))
```

According to the result_grouped table, k nearest neighbors values of `r best_k_neighbors` gave the most accurate results via regular validation with an accuracy of `r best_accuracy`

Testing the model we picked on the testing data we portioned out at the beginning. 3. report the picked model's accuracy as its performance on regular_test_data

```{r, knn regular test,message=FALSE}
regular_test_accuracy <- knn_accuracy(training_data = regular_train_data,
                                 testing_data = regular_test_data,
                                 k = best_k_neighbor,
                                 kernel = "optimal")
regular_test_accuracy
best_accuracy
#difference between validation accuracy and test accuracy
accuracy_diff <- abs(best_accuracy - regular_test_accuracy )
accuracy_diff
```

It was expected that the true model accuracy `r regular_test_accuracy` is lower than the training accuracy `r best_mean_accuracy`. It's of interest to note that the k-folds cross validation's validation accuracy is closer to it's test accuracy than the regular validation's validation accuracy to its test accuracy. K-fold cross validation generally provides a more robust estimate of model performance because it averages results over multiple train-test splits, 10 in this case. And it utilizes the data efficiently by using each data point for both training and testing at some point. However, this is more useful for smaller datasets like ours because it can be computationally expensive.
