---
title: "knn_model"
output:
  html_document: default
  word_document:
    fig_caption: yes
    fig_height: 5
    fig_width: 5
    reference_docx: hw_template.docx
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kernlab)
library(dplyr)
library(tidyverse)
library(kknn)
library(ggplot2)
library(GGally)
library(factoextra)
```

importing data
```{r importing data,results='hide',message=FALSE,cache=TRUE}
cc_data <-read_tsv(file = "../data 3.1/credit_card_data-headers.txt",col_names = TRUE)
```

### Q3.1 Using the same data set as in Question 2.2, use the ksvm or kknn function to find a good classifier: (a)	using cross-validation (do this for the k-nearest-neighbors model; SVM is optional)

I'll be doing K-folds cross validation to find a good classifier for KNN model

1. creating the training/validation and testing datasets and set the final_test_data aside to the test the model picked by K-fold CV.
```{r}
#let's set a seed for reproducability
set.seed(1234) #random number
#first, separate the dataset into 80/20 (80% training and validation for cross validation and 20% for test)
n_dp <- nrow(cc_data)
split_value <- 0.80
#we are randomly shuffling the entire dataset and then splitting it up according the the split_value we set above.
training__valid_data_points <- sample(x=1:n_dp,size = as.integer(split_value*n_dp),replace = FALSE)
#and then subsetting the data as such
train_valid_data <- cc_data[training__valid_data_points,]
final_test_data <- cc_data[-training__valid_data_points,]
```
2. Set up k-folds cross validation. In lecture, professor mentioned k=10 is a good value to use. It’s not necessarily always the most optimal but smaller values of k (e.g <5) can lead to higher variance in performance estimate because the evaluation is based on fewer data points which larger k’s (>20) can lead to higher bias in the estimate because each fold contains a smaller portion of the data.

First, I define the function for running knn and obtaining the accuracy.
```{r, knn,message=FALSE}
# define the function for running KNN and returning the accuracy value
knn_accuracy <- function(training_data, testing_data, k, kernel){
formula <- as.factor(R1) ~ A1 + A2 + A3 + A8 + A9 + A10 + A11 + A12 + A14 + A15
    #running the model/prediction using the predefined inputs
    kknn_model <- kknn(formula = formula, training_data, testing_data,
                         k = k, kernel = kernel, scale=TRUE)
    #getting the fitted value
    fitted_value <- kknn_model$fitted.values
    #getting accuracy and returning it
    accuracy_kknn <- sum(fitted_value == testing_data$R1)/ nrow(testing_data)
    return(accuracy_kknn)
}
```

Then I define the k-folds function. This returns a table that contains each k nearest neighbor hyperparameter, the k-folds validation, and accuracy per portion of k-folds validation. This table is then averaged on the accuracy across k-folds evaluations. Then, we can see what the average accuracy of each model is to determine which knn k value is best to use. I will be using k-folds=10 as stated above and testing knn models using k nearest neighbor values from 1-20. 
```{r}
kfolds_CV <- function(k_folds,train_valid_data,k_neighbors, kernel){
  #create a df to store the results
  results <- data.frame()
  #shuffling the dataset
  set.seed(1234) #random number
  train_valid_data<-train_valid_data[sample(nrow(train_valid_data)),]
  #segmenting my training/validating data into k folds.
  folds <- cut(seq(1,nrow(train_valid_data)),breaks=k_folds,labels=FALSE)
  #determine the loop for how many k_neighbors I want to try
  for(kn in 1:k_neighbors){
      #Perform k fold cross validation
     for(kf in 1:k_folds){
      #Segment data by fold using the which() function 
      test_indexes <- which(folds==kf,arr.ind=TRUE)
      #define the test data per the indices defined above
      test_data <- train_valid_data[test_indexes, ]
      #same for train data
      train_data <- train_valid_data[-test_indexes, ]
      #run the knn function and get the accuracy value for the above sets
      knn_res <- knn_accuracy(train_data,test_data,
                              k = kn,kernel = "optimal")
      #staging the results to add to final df
      result <- data.frame(current_k_fold=kf, 
                           k_neighbor=kn, 
                           accuracy=knn_res)
      #add results to final df
      results <- bind_rows(results,result)
     }
  }
  return(results)
}
```

run the kfolds CV function above with parameters: k-folds=10, on the training/validating data set (80% of whole dataset), and using k of 1-20 for knn model with the kernel 'optimal'.
```{r, fig.height=4,fig.width=6}
model <- kfolds_CV(k_folds = 10,
                  train_valid_data = train_valid_data,
                  k_neighbors = 20,
                  kernel = "optimal")

#take the average of all 10 evaluations per k neighbor value and sort by highest mean accuracy
result <- model %>%
  group_by(k_neighbor) %>%
  summarize(mean_accuracy = mean(accuracy)) %>% 
  arrange(desc(mean_accuracy))

#group the mean accuracy value by k neighbors
result_grouped <- result %>%
  group_by(mean_accuracy) %>%
  summarize(k_neighbors_list = paste0(k_neighbor,collapse = ", ")) %>% 
  arrange(desc(mean_accuracy))
result_grouped

#save these values for testing below.
best_k_neighbor <- result$k_neighbor[1]
best_mean_accuracy <- result$mean_accuracy[1]
best_k_neighbors <- result_grouped$k_neighbors_list[1]

#plot the results
ggplot(result, aes(x = k_neighbor, y = mean_accuracy, color = k_neighbor, label = k_neighbor)) + geom_point() + ggtitle("K-Folds Cross Validation Accuracy") +
  labs(x = "K-nearest neighbor", y = "Mean Accuracy") + theme_minimal() +
  theme(legend.position = "none") +   scale_x_continuous(breaks = scales::pretty_breaks(n = 20))
```

So according to the "result_grouped" table, we can see that the k values for kknn that performed the best via 10-folds cross validation were `r best_k_neighbors` with a mean accuracy across 10-folds of `r best_mean_accuracy`. We will pick this model. Moving on to the final portion of the cross validation phase, evaluating this model on the testing data we portioned out at the beginning.

3. Train chosen model on all of train_valid_data and evaluate on test_data. Then report the accuracy as it is performance on test_data.
```{r}
cv_test_accuracy <- knn_accuracy(training_data = train_valid_data,
                                 testing_data = final_test_data,
                                 k = best_k_neighbor,
                                 kernel = "optimal")
cv_test_accuracy
best_mean_accuracy

accuracy_diff <- abs(best_mean_accuracy - cv_test_accuracy)
accuracy_diff
```
It was expected that the true model accuracy `r cv_test_accuracy` is lower than the training accuracy `r best_mean_accuracy`...however it's still rather close being `r accuracy_diff` off from the training accuracy. I'm satisfied with choosing k value of `r best_k_neighbor` for this classifier.


### (b)	splitting the data into training, validation, and test data sets (pick either KNN or SVM; the other is optional).
1. creating the training,validation,and testing datasets. I want the majority of the data for training so I’ll do 60% for training, 20% for validation, and 20% for test. First I split it up into 60/40. Then I split that last 40 in half for validation and test.
```{r, regular validation, message=FALSE}
#let's set a seed for reproducability
set.seed(1234) #random number

#first, separate the dataset into 60/20/20 (60% training, 20% validation, and 20% for test)
n_dp <- nrow(cc_data)
training_split_value <- 0.60

#we are randomly shuffling the entire dataset and then splitting it up according the the training_split_value we set above.
training_data_points <- sample(x=1:n_dp,
                               size = as.integer(training_split_value*n_dp),
                               replace = FALSE)

#and then subsetting the data as such
regular_train_data <- cc_data[training_data_points,]
fourty_perc_of_data <- cc_data[-training_data_points,]

#and then divide the rest of the data evenly between validation and test
n_rest_of_data <- nrow(fourty_perc_of_data)
valid_split_value <- 0.5
set.seed(1234) #set seed again so I can reproduce this later
valid_data_points <- sample(x=1:n_rest_of_data,
                            size = as.integer(valid_split_value*n_rest_of_data),
                            replace = FALSE)

regular_valid_data <- fourty_perc_of_data[valid_data_points,]
regular_test_data <- fourty_perc_of_data[-valid_data_points,]
```

2. train all models on regular_train_data and validate all models on regular_valid_data and pick the best model
```{r, knn regular validation,message=FALSE, fig.height=4,fig.width=6}
#define function to run regular cross validation
regular_v <- function(train_data,validation_data,k_neighbors, kernel){
  #create a df to store the results
  results <- data.frame()
  #determine the loop for how many k_neighbors I want to try
  for(kn in 1:k_neighbors){
      #run the knn function and get the accuracy value for validation set
      knn_res <- knn_accuracy(train_data,validation_data,
                              k = kn,kernel = kernel)
      #staging the results to add to final df
      result <- data.frame(k_neighbor=kn, accuracy=knn_res)
      #add results to final df
      results <- bind_rows(results,result)
  }
  return(results)
}
#run the regular validation function above with parameters: k-folds=10, on the training/validating data set (80% of whole dataset), and using k of 1-20 for knn model.
model <- regular_v(train_data = regular_train_data,
                    validation_data = regular_valid_data,
                    k_neighbors = 20,
                    kernel = "optimal")

result <- model %>%
  arrange(desc(accuracy))

#group the accuracy value by k neighbors
result_grouped <- result %>%
  group_by(accuracy) %>%
  summarize(k_neighbors_list = paste0(k_neighbor,collapse = ", ")) %>% 
  arrange(desc(accuracy))
result_grouped

#save these values for testing below.
best_k_neighbor <- result$k_neighbor[1]
best_accuracy <- result$accuracy[1]
best_k_neighbors <- result_grouped$k_neighbors_list[1]

#plot the results
ggplot(result, aes(x = k_neighbor, y = accuracy, color = k_neighbor, label = k_neighbor)) +
  geom_point() + ggtitle("Regular Validation Accuracy") + 
  labs(x = "K-nearest neighbor", y = "Accuracy") + theme_minimal() +
  theme(legend.position = "none") + 
  scale_x_continuous(breaks = scales::pretty_breaks(n = 20))
```

According to the result_grouped table, k nearest neighbors values of `r best_k_neighbors` gave the most accurate results via regular validation with an accuracy of `r best_accuracy`

Testing the model we picked on the testing data we portioned out at the beginning.
3. report the picked model's accuracy as its performance on regular_test_data
```{r, knn regular test,message=FALSE}
regular_test_accuracy <- knn_accuracy(training_data = regular_train_data,
                                 testing_data = regular_test_data,
                                 k = best_k_neighbor,
                                 kernel = "optimal")
regular_test_accuracy
best_accuracy
#difference between validation accuracy and test accuracy
accuracy_diff <- abs(best_accuracy - regular_test_accuracy )
accuracy_diff
```
It was expected that the true model accuracy `r regular_test_accuracy` is lower than the training accuracy `r best_mean_accuracy`. It's of interest to note that the k-folds cross validation's validation accuracy is closer to it's test accuracy than the regular validation's validation accuracy to its test accuracy. K-fold cross validation generally provides a more robust estimate of model performance because it averages results over multiple train-test splits, 10 in this case. And it utilizes the data efficiently by using each data point for both training and testing at some point. However, this is more useful for smaller datasets like ours because it can be computationally expensive.
